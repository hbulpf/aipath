@[toc]

一直知道交叉熵用于多元分类（每张图只属于一个类别）任务的损失函数，但是一直没有细纠其中的原因，因此有了本篇博客
# 什么是交叉熵
在此之前，我们先来认识一下KL散度，假设有两个概率分布$p(x),q(x)$，$p(x)$为真实的概率分布，$q(x)$为模型所表示的概率分布，对于离散型随机变量而言（分类问题通常是离散型随机变量），假设离散型随机变量有n个取值(可以有重复取值)，则KL散度（相对熵）定义为$$KL(p(x)||q(x))=\sum_{i=1}^{n}p(x_i)log(\frac{p(x_i)}{q(x_i)})\tag{式1.0}$$
连续型随机变量的KL散度定义为
$$KL(p(x)||q(x))=\int_xp(x)log(\frac{p(x)}{q(x)})$$

相对熵用于描述$p(x)$与$q(x)$的差距，即如果用$p(x)$来描述样本，那么就非常完美。而用$q(x)$来描述样本，虽然可以大致描述，但是不是那么的完美，信息量不足，需要额外的一些“信息增量”才能达到和p(x)一样完美的描述，对于分类问题而言，我们自然希望相对熵最小化，此时模型所表示的概率分布与真实的概率分布差距最小，对相对熵进行变化得
$$\begin{aligned}
\sum_{i=1}^np(x_i)log(\frac{p(x_i)}{q(x_i)})=\sum_{i=1}^np(x_i)log(p(x_i))-\sum_{i=1}^{n}p(x_i)log(q(x_i))
\end{aligned}
$$
$\sum_{i=1}^{n}p(x_i)log(p(x_i))$取值为常数，则最小化KL散度等价于最小化$$-\sum_{i=1}^np(x_i)log(q(x_i))\tag{式1.1}$$
式1.1即为交叉熵，连续型随机变量也有类似的推导

<br>

# 极大似然估计与交叉熵的关系
分类问题中的极大似然估计的结果就是交叉熵的形式，设模型对应的概率密度函数为$$P_G(x;w)$$
$w$为待估参数，设有m个服从$P_{data}$分布的样本$x_1,x_2,....,x_m$,则似然函数记为
$$\prod_{i=1}^{m}P_G(x_i;w)\Delta x\tag{式2.0}$$
则对数似然函数记为
$$\sum_{i=1}^{m}[log(P_{G}(x_i;w))+log\Delta x]\tag{式2.1}$$

极大似然估计的目标为
$$
\arg\max\limits_w\sum_{i=1}^{m}log(P_{G}(x_i;w))\tag{式2.2}
$$

由于样本的均值可以作为总体分布均值的无偏估计，且$E(f(x))=\int_{-\infty}^{+\infty}f(x)g(x)dx$，x的概率密度函数为$P_{data}(x)$，因此有
$$
\begin{aligned}
\arg\max\limits_w\sum_{i=1}^{m}log(P_{G}(x_i;w))&=\arg\max\limits_w\frac{1}{m}\sum_{i=1}^{m}log(P_{G}(x_i;w))\\
&\approx \arg\max\limits_w E(log(P_{G}(x;w))\\
&=\arg\max\limits_w\int_{x}P_{data}(x)log(P_{G}(x;w)\\
&=\arg\min\limits_w-\int_{x}P_{data}(x)log(P_{G}(x;w))\\
&\approx \arg\min\limits_w-\sum_{i=1}^mP_{data}(x_i)logP_G(x_i;w)\Delta x^2\\
&=\arg\min\limits_w-\sum_{i=1}^mP(x_i)logQ(x_i;w)
\end{aligned}
$$
极大似然估计约等于最小化交叉熵

<br>

# 分类损失函数

## 单标签分类
单标签分类是一个$x$对应一个标签$y$的情况，类似于一张图片只含有猫，则网络应该将这张图片划分为猫类，此类问题也可以通过极大似然估计解决。

假设现在有$n$个类别，由于一张图片只能属于一个类别，所以这$n$个类别互斥，设$A_i$表示$x$属于第$i$类，$\hat A_i$表示$x$不属于第$i$类，现有一个属于第一类的实例$x$，则实例x发生的概率为

$$P(A_1\hat A_2\hat A_3.....\hat A_n)=P(A_1)$$

我们有$m$个实例，$m$个实例之间相互独立，第$i$类有$n_i$个样本，则对数似然估计函数为

$$
n_1 logP(A_1)+n_2 logP(A_2).....+n_n logP( A_n)\tag{式3.1}
$$

设第$i$个样本的one-hot编码为$y_i$，$q_j(x,w)$表示x属于第$j$类的概率，样本$x_i$对应各类别概率的对数（$log\ q_j(x_i,w)$）组成的向量为$log\ q(x_i,w)$，$w$为待估参数，则式3.1可更改为

$$
\sum_{i=1}^{m}y_i\log q(x_i,w)\tag{式3.2}
$$ 
式3.2即为单标签分类的损失函数，形式上与交叉熵一致

<br>

## 多标签分类

多标签分类是一个$x$对应多个标签$y$的情况，类似于一张图片中含有猫和狗，则网络应该将这张图片即划分为猫类，也划分为狗类。此类问题的参数估计可以通过对数极大似然估计解决。

多标签分类可以看成是多个二分类问题叠加的结果。假设现在有$n$个类别，每个类别之间相互独立，设$A_i$表示$x$属于第$i$类，$\hat A_i$表示$x$不属于第$i$类，现有一个实例$x$，属于第一类与第二类，则实例$x$发生的概率为
$$P(A_1A_2\hat A_3\hat A_4...\hat A_n)=P(A_1)P(A_2)P(\hat A_3)P(\hat A_4)...P(\hat A_n)$$
则有
$$
\begin{aligned}
\log P(A_1A_2\hat A_3\hat A_4...\hat A_n)=&\log P(A_1)+\log P(A_2)+\log P(\hat A_3)+\log P(\hat A_4)...+\log P(\hat A_n)\\
=&\sum_{j=1}^{n}[y_j\log P(A_j)+(1-y_j)(1-\log P(A_j))] \\
&(y_i \in \{0,1\})
\end{aligned}$$

设$q_i(x;w)$表示$x$属于第$i$类的概率，$w$为待估参数，则对应的对数似然函数为
$$\sum_{i=1}^m\sum_{j=1}^{n}[y_j\log q_j(x_i;w)+(1-y_j)(1-\log q_j(x_i;w))]\tag{式3.3}$$ 

通过使用对数极大似然估计，多标签分类任务中的目标为$$\arg \min\limits_w-\sum_{i=1}^m\sum_{j=1}^{n}[y_j\log q_j(x_i;w)+(1-y_j)(1-\log q_j(x_i;w))]$$

网络上有部分文章认为交叉熵可用于多标签分类任务，个人不这么认为，交叉熵应该只能用于单标签分类。

