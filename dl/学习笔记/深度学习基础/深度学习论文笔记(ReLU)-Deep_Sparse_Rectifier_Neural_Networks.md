@[toc]

# 主要工作
提出了ReLU激活函数，ReLU激活函数的表现能力比tanh激活函数的更加好，训练完毕的神经网络具有一定的稀疏性

人工神经网络与生物神经网络有一定的相似性，例如CNN与生物视觉神经处理数据的形式往往相似
<br>

# 人工神经网络与生物神经网络的区别

 一、大脑神经元通过稀疏且分布的方式编辑信息。什么是稀疏与分布？我的理解是，只有少部分神经元(研究显示只有1%到4%的神经元参与信息编辑)会参与信息的编辑，并且这些神经元的层级往往不一致，例如A神经元将B神经元的输出作为输入。在没有L1范式的情况下，使用sigmoid与tanh的人工神经网络往往不具有稀疏性。
 
 二、生物神经网络与人工神经网络的激活函数形式差异较大，如下图：
 ![在这里插入图片描述](https://img-blog.csdnimg.cn/20190826155220787.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RoYWl1ZGE=,size_16,color_FFFFFF,t_70) 

基于上述两点，我们有了ReLU激活函数，如下图：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20190826155649606.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RoYWl1ZGE=,size_16,color_FFFFFF,t_70)
可以看到，ReLU激活函数在形式上更加接近生物神经网络。当神经元的输入大于0时，神经元才有输出，否则，神经元输出为0，即神经元死亡，通过ReLU训练的神经网络往往具有很好的稀疏性，只有一部分神经元会参与计算，从而更好的模拟了生物神经网络

<br>

# 稀疏性带来的优势
这部分我没有读的很懂，在此做个简单总结

一、如果神经网络是稠密的，即神经元的输出都大于0，那么输入的一个微小改变，都可能在一次又一次的前向传播中被放大，导致神经网络的输出发生很大变化，这就会导致整个神经网络不够健壮，若一个神经网络是稀疏的，即部分神经元的输出为0，输入的微小改变也许就不会在一次又一次的前向传播中被放大，导致神经网络输出发生大的变化。

二、不同的输入蕴含的信息不同，如果我们想用一个数据结构去表示信息，则这种数据结构的大小应该是可变的，否则对于某些输入就无法进行表示，如果一个神经网络是稀疏的，我们就可以通过激活的神经元数量来表示不同大小的信息

三、线性可分，如果数据是稀疏的，则可能是线性可分的

四、对于神经网络而言，不论是稀疏亦或是稠密，模型都具有一定的复杂度，仍然可以拟合复杂的数据，但过分稀疏会导致模型的复杂度下降，模型的拟合能力大大下降

插句题外话，一般来说，为了提高模型的复杂度，CNN神经网络采用的是加宽与加深网络，但是网络的加深也会带来一些问题，例如难以收敛、梯度消失等，但是已经有相应的解决方案了，例如BN算法、残差结构、一系列的初始化方法

<br>

# ReLU神经元的优势
一、带来的神经网络稀疏性不仅与生物神经元更加相似，而且在数学上具有一定优势（见上一节）
二、虽然每个神经元都是线性输出，但对于给定输入，神经网络的非线性可通过是否激活某个神经元体现
三、若神经元被激活，则激活函数的导数为1，梯度在激活的神经元之间传输不会出现梯度消失的情况，但是总体仍然可能出现梯度消失，此时大量神经元死亡，网络无法更新，loss函数无法进一步下降，可以考虑使用MSRA初始化
四、由于不需要进行指数等费时的运算，计算速度快

<br>

# ReLU神经元的劣势
一、有些学者可能假设ReLU在x<0处取0不利于优化，但是通过实验，发现ReLU有利于有监督网络的优化
二、由于激活函数是没有上界的，有可能出现神经网络输出为NaN的情况
三、虽然优化时，权重与偏差会偏移不同的尺度，但作用在一起，整个神经网络的输出可能是不变的，具体一点，举个例子，假设有三层神经网络，对于某一个输入的输出为$s$，更新后，第$i$层神经元的权重为$\frac{w_i}{\alpha_i}$，偏差为$\frac{b_i}{\prod_{j=1}^i \alpha_j}$，则对同一输入，神经网络的输出为
$$
\frac{w_3}{\alpha_3(}\frac{w_2}{\alpha_2}(\frac{w_1}{\alpha_1}x+\frac{b_1}{\alpha_1})+\frac{b_2}{\alpha_1\alpha_2})+\frac{b_3}{\alpha_1\alpha_2\alpha_3}=(\frac{1}{\alpha_1\alpha_2\alpha_3})*s
$$
若$\frac{1}{\alpha_1\alpha_2\alpha_3}=1$，则更新前后神经网络的输出一致




