
@[toc]

# 前言
寒假忙一个会议论文去了，到现在才有时间更新博客，大四生活和想像的不一样，一如既往的多事做（捂脸。

本文所有深度学习技巧均针对于图像识别模型。

训练深度学习模型需要大量数据，这将导致需要花费大量的时间训练模型，本文将总结加速深度学习模型收敛的技巧，同时将总结如何让深度学习模型尽可能收敛到一个好的解，从而提高模型的泛化能力。

<br>

# 如何加速模型收敛
本节将总结如何在不影响模型准确率的前提下，加速模型训练的技巧。

<br>

## 增大batch size
当前大部分深度学习模型都使用Minibatch SGD，或是使用BN算法。**增大batch size一定程度上可以加速模型收敛，提高模型的泛化能力，但是batch size过大会影响模型的泛化性能与收敛速度，因此batch size并不是越大越好，并且batch size的大小会影响初始学习率的选择，而初始学习率会影响到模型的收敛速度**。

《Bag of Tricks for Image Classification with Convolutional Neural Networks》一文总结了batch size对模型收敛与泛化能力的影响，结论：**batch size在一定范围时，让模型稳定收敛的初始学习率具有较大的选择空间，并且可以提高泛化性能**，该论文做了较多实验，这里截取两张支撑上述结论的结果

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200328090504770.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RoYWl1ZGE=,size_16,color_FFFFFF,t_70#pic_center)
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200328090522282.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RoYWl1ZGE=,size_16,color_FFFFFF,t_70#pic_center)
上面两幅图，右图表明batch size在一定范围时，使模型收敛的初始学习率的选择范围较广，并且初始学习率大小会影响到模型的泛化性能，当batch size过大时，初始学习率的选择范围较窄，并且模型的泛化性能下降。左图表明batch size过大，会导致模型的泛化性能下降。

<br>

## Linear scaling learning rate
从上一节可知，增大batch，可以加速模型收敛速度，但是大batch会导致模型的泛化性能下降。如果让大batch一次梯度更新后的权重与小batch $k$次梯度更新后的权重一致，那么就能在加速模型收敛的同时，防止模型泛化性能下降，这便是**Linear scaling learning rate**的出发点。

**Linear scaling learning rate**是指当batch size增大n倍时，learning rate也要增大n倍。该理论的推导过程如下：

设小batch size为$m$，大batch size为$nm$，$L(\theta)$表示参数为$\theta$时的损失函数。小batch在n次梯度更新后，模型权重$\theta_{k+n}^{small}$为
$$
\theta_{k+n}^{small}=\theta_{k}^{small}-\frac{\alpha}{m} \sum_{j=1}^n\sum_{i=1}^m\nabla_{\theta}L_{i+jm}(\theta_{k+j})\tag{1.0}
$$

大batch在1次梯度更新后，模型权重$\theta_{k+1}^{large}$为
$$
\theta_{k+1}^{large}=\theta_{k}^{large}-\frac{\alpha'}{nm}\sum_{i=1}^{nm}\nabla_{\theta}L_{i}(\theta_{k})\tag{2.0}
$$

如果让
$$
\sum_{j=1}^n\sum_{i=1}^m\nabla_{\theta}L_{i+jm}(\theta_{k+j}) \approx \sum_{i=1}^{nm}\nabla_{\theta}L_{i}(\theta_{k})\tag{3.0}
$$

$$
\alpha'=\alpha*n\tag{4.0}
$$

则有$\theta_{k+1}^{large} \approx \theta_{k+n}^{small}$，注意到batch size越大，式3.0越难成立，因此**Linear scaling learning rate**只在一定batch size范围内可能有效。

<br>

## learning rate warmup
如果参数变化非常剧烈，那么式3.0将难以满足，而在训练初期，由于参数随机化，参数离真正的解差距较大，模型的参数变化最为剧烈，为了防止训练初期参数变化过于剧烈，研究人员提出了**learning rate warmup**。顾名思义，warm up即让模型在进行正式训练前，先使用小的学习率迭代一定epoch，即热身，通过热身降低参数离真正的解的差距，从而防止训练初期参数变化过于剧烈。

warm up有两种策略：

1、**Constant warmup**：使用固定的学习率$\alpha'$，迭代一定次数后，改为正式训练的学习率$\alpha$，在目标检测与语义分割领域有很好的应用（部分pretrain层与随机初始化层一起训练，而非全部随机初始化）
2、**Gradual warmup**：设warm up的迭代次数为$m$，epoch为$i$($0\leq i\leq m$)，则第$i$个epoch使用的学习率为$\frac{(i*\alpha)}{m}$，经过m个epoch后，学习率大小变为正式训练的学习率。

<br>

## Zero $γ$
在BN算法中，设经过标准化的特征图为$\hat{x}$，则BN层的输出为$γ\hat{x}+\beta$，Zero $γ$即在初始化时，将$γ$设置为0。该启发式方案只针对ResNet，此时残差结构的输出等于输入（可以看成是一层），在训练初期，Zero $γ$相当于降低了网络层数，而浅层网络往往较容易训练。

<br>

## No bias decay
weight decay会对网络的所有超参数施加正则化，例如BN层的$γ、\beta$、卷积层与全连接层的bias，No bias decay建议只对卷积层与全连接层的权重施加L2正则化，No bias decay降低了weight decay的正则化程度，这在一定程度上会加速网络的收敛。

<br>

## 实验
上述操作可以在一定程度上加速网络训练，但是否会大幅度影响网络泛化能力呢？2019年CVPR论文《Bag of Tricks for Image Classification with Convolutional Neural Networks》对此进行了实验，结果如下：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200328160513899.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RoYWl1ZGE=,size_16,color_FFFFFF,t_70#pic_center)
利用**Linear scaling**，增大batch后，网络的泛化性能会略有下降，但在此基础上stack其他技巧，测试准确率不会出现非常严重的下滑，表明这些策略在加速网络训练你的同时，对网络泛化能力的影响不大。

<br>

# 如何提高模型的准确率
本节将总结固定其他一切训练配置（例如使用的网络、batch size）的前提下，提高模型准确率的技巧

<br>

## Cosine Learning Rate Decay
训练网络时，一般每隔一定epoch数后，会将learning rate降低一定倍数，这被称为step decay。假设batch的个数为$T$（数据总数除以batch size），初始学习率为$\alpha$，则在第t个batch，Cosine Learning Rate Decay的大小为
$$
\alpha'=\frac{1}{2}(1+\cos\frac{t\pi}{T})\alpha
$$

假设我们使用了warm up策略，则step decay与Cosine Learning Rate Decay的学习率变化与准确率变化率曲线为
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200328163319827.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RoYWl1ZGE=,size_16,color_FFFFFF,t_70#pic_center)
可以看到，相比于step decay，Cosine Learning Rate Decay在初期训练，学习率下降速度迅猛，在中期下降速度与epoch呈线性趋势，后期下降速度减缓，准确率曲线上升较为平缓有序。step decay的准确率曲线与学习率下降曲线波动都较大。

<br>

## Label Smoothing
在分类任务中，常通过将fc层输出经过softmax处理后，通过最小化交叉熵损失函数进行优化。图像的label为hard label（即one hot），此时最小化交叉熵相当于鼓励fc层的target score远大于other score，例如图像的label为狗，则最小化交叉熵相当于鼓励fc层表示狗的神经元，其score远大于其他类别的score，如果数据集太小，这有可能导致过拟合。为了解决上述问题，**Label Smoothing**提倡smooth hard label，将hard label更改为：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200328173909268.png#pic_center)
$\varepsilon$为一个超参数，取值(0,1)，$K$为类别总数。设第i个类别，fc层的输出为$z_i$，则使损失函数最小的$z_i$（其为凸函数，推导过程略）变为：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200328174133407.png#pic_center)
$\alpha$可以是任意值，$K$为类别总数。可见，使用**Label Smoothing**后，target score与other score之间的差距将为$\log((K-1)(1-\varepsilon)/\varepsilon)$，该差距的大小与$\varepsilon$的关系如下：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200328174755633.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RoYWl1ZGE=,size_16,color_FFFFFF,t_70#pic_center)

<br>

## Mixup Training
Mixup Training相当于在数据集中加入一些噪声，个人理解为一种数据增强。我们随机采样两张图片，记录为$(x_i,y_i)、(x_j,y_j)$，我们将这两张图片通过下列方式合并为一张新图片$(\hat x,\hat y)$：
$$
\begin{aligned}
\hat x&=\lambda x_i+(1-\lambda) x_j\\
\hat y&=\lambda y_i+(1-\lambda) y_j
\end{aligned}
$$

$\lambda$的取值为(0,1)，为$Beta(\alpha,\alpha)$分布随机采样获得。

<br>


# 参考资料
[1] Priya Goyal，Piotr Dolla ́r，Ross Girshick，Pieter Noordhuis.Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour.CoRR 2017
[2] Tong He，Zhi Zhang，Hang Zhang，Zhongyue Zhang，Junyuan Xie，Mu Li
.Bag of Tricks for Image Classification with Convolutional Neural Networks.CVPR 2019
[3] Dominic Masters，Carlo Luschi. Revisiting small batch training for deep neural networks.CVPR 2018


![在这里插入图片描述](https://img-blog.csdnimg.cn/20200404200312425.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RoYWl1ZGE=,size_16,color_FFFFFF,t_70#pic_center)
