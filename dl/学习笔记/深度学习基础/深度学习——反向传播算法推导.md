@[toc]

# 前言
之前写过单层前馈神经网络，但是其中的推导是针对sigmoid函数的，本篇博客使用矩阵向量求导方式进行反向传播算法的推导

<br>

# 符号约定
|符号|含义  |
|--|--|
| $S_{in}^i$ | 第$i$层神经元的输入，若一层有n个神经元，则$S_{in}^i$是一个$n*1$的向量 |
| $S_{out}^i$ | 第$i$层神经元的输出，若一层有n个神经元，则$S_{out}^i$是一个$n*1$的向量 |
|$W^i$|第$i$层神经元对应的权重矩阵，若$i-1$层有$m$个神经元，第$i$层有$n$个神经元，则$W^i$为$n*m$的矩阵|
|$B^i$|第$i$层的偏移矩阵，若一层有n个神经元，则$B^i$是一个$n*1$的向量|
|$cost$|损失函数值|

若$x$表示$\begin{bmatrix}
x_1\\
x_2\\
....\\
x_n
\end{bmatrix}$，第i层的激活函数向量$f^i(x)$表示为$\begin{bmatrix}
f(x_1)\\
f(x_2)\\
....\\
f(x_n)
\end{bmatrix}$，$f(x)$为激活函数，$(f^i(x))'$表示为$\begin{bmatrix}
\frac{\partial{f(x_1)}}{{\partial x_1}}\\
\frac{\partial{f(x_2)}}{{\partial x_2}}\\
....\\
\frac{\partial{f(x_n)}}{{\partial x_n}}
\end{bmatrix}$

<br>

基于上述符号约定，对于第$i$层的神经元，我们有
$$
\begin{aligned}
S_{out}^{i-1}=&f^i(S_{in}^{i-1})\\
S_{in}^i=&W^iS_{out}^{i-1}+B^i
\end{aligned}
$$

<br>

# 标量对向量求导的链式法则

对于$n$层前馈神经网络，我们有
$$cost\leftarrow S_{in}^n\leftarrow S_{in}^{n-1}.....\leftarrow S_{in}^1$$
左箭头表示映射，对于前馈神经网络，映射即为
$$
\begin{aligned}
S_{in}^{i+1}=W^{i+1}f^{i}(S_{in}^{i})+B^{i+1}
\end{aligned}
$$
损失函数与最后一层的映射需要依据损失函数的类型决定（例如均方误差、交叉熵），在上述映射关系的基础上，标量对向量求导的链式法则定义为
$$
\begin{aligned}
\frac{\partial cost}{\partial S_{in}^i}&=(\frac{\partial S_{in}^n}{\partial S_{in}^{n-1}}*\frac{\partial S_{in}^{n-1}}{\partial S_{in}^{n-2}}*.......*\frac{\partial S_{in}^{i+1}}{\partial S_{in}^{i}})^T*\frac{\partial cost}{\partial S_{in}^n}\\
&=(\frac{\partial S_{in}^{i+1}}{\partial S_{in}^{i}})^T*.....*(\frac{\partial S_{in}^{n-1}}{\partial S_{in}^{n-2}})^T*(\frac{\partial S_{in}^n}{\partial S_{in}^{n-1}})^T*\frac{\partial cost}{\partial S_{in}^n}\\
&=(\frac{\partial S_{in}^{i+1}}{\partial S_{in}^{i}})^T*.....*(\frac{\partial S_{in}^{n-1}}{\partial S_{in}^{n-2}})^T*\frac{\partial cost}{\partial S_{in}^{n-1}}\\
&=.......\\
&=(\frac{\partial S_{in}^{i+1}}{\partial S_{in}^{i}})^T\frac{\partial cost}{\partial S_{in}^{i+1}}
\end{aligned}
$$


<br>

## 常用向量对向量求导的公式
若$Y=AX+B$，$Y$、$X、B$为向量，$A$为矩阵，使用分子布局，则有$$\frac{\partial{Y}}{\partial{X}}=A$$

<br>

## 反向传播算法推导
假设有一个n层前馈神经网络，则第$i$层的梯度为
$$
\begin{aligned}
\frac{\partial cost}{\partial S_{in}^i}&=(\frac{\partial S_{in}^{i+1}}{\partial S_{in}^{i}})^T*\frac{\partial cost}{\partial S_{in}^{i+1}}\\
&=(W^{i+1})^T*\frac{\partial cost}{\partial S_{in}^{i+1}}  ☉ (f^{i}(S_{in}^i))'
\end{aligned}\tag{式1}
$$
☉为Hadamard乘积，用于矩阵或向量之间点对点的乘法运算，即相同位置的元素相乘，对于最后一步，具体的理解如下，假设第$i$层有n个神经元
$$
\begin{aligned}
(W^{i+1})^T*\frac{\partial cost}{\partial S_{in}^{i+1}}☉ (f^{i}(S_{in}^i))'=&(\frac{\partial S_{in}^{i+1}}{\partial f(S_{in}^{i})})^T*\frac{\partial cost}{\partial S_{in}^{i+1}}☉ (f^{i}(S_{in}^i))'\\
=&\frac{\partial cost}{\partial f(S_{in}^{i})}☉ (f^{i}(S_{in}^i))'\\
=&
\begin{bmatrix}
\frac{\partial cost}{\partial f((S_{in}^{i})_1)}\\
\frac{\partial cost}{\partial f((S_{in}^{i})_2)}\\
......\\
\frac{\partial cost}{\partial f((S_{in}^{i})_n)}
\end{bmatrix}☉ (f^{i}(S_{in}^i))'\\
=&
\begin{bmatrix}
\frac{\partial cost}{\partial f((S_{in}^{i})_1)}\\
\frac{\partial cost}{\partial f((S_{in}^{i})_2)}\\
......\\
\frac{\partial cost}{\partial f((S_{in}^{i})_n)}
\end{bmatrix}☉
\begin{bmatrix}
\frac{\partial {f((S_{in}^{i})_1)}}{\partial((S_{in}^{i})_1)}\\
\frac{\partial {f((S_{in}^{i})_2)}}{\partial((S_{in}^{i})_2)}\\
......\\
\frac{\partial {f((S_{in}^{i})_n)}}{\partial((S_{in}^{i})_n)}
\end{bmatrix}\\
=&\begin{bmatrix}
\frac{\partial cost}{\partial f((S_{in}^{i})_1)}*\frac{\partial {f((S_{in}^{i})_1)}}{\partial((S_{in}^{i})_1)}\\
\frac{\partial cost}{\partial f((S_{in}^{i})_2)}*\frac{\partial {f((S_{in}^{i})_2)}}{\partial((S_{in}^{i})_2)}\\
......\\
\frac{\partial cost}{\partial f((S_{in}^{i})_n)}*\frac{\partial {f((S_{in}^{i})_n)}}{\partial((S_{in}^{i})_n)}
\end{bmatrix}\\
=&\begin{bmatrix}
\frac{\partial cost}{\partial((S_{in}^{i})_1)}\\
\frac{\partial cost}{\partial((S_{in}^{i})_2)}\\
......\\
\frac{\partial cost}{\partial((S_{in}^{i})_n)}
\end{bmatrix}\\
=&\frac{\partial cost}{\partial S_{in}^i}
\end{aligned}
$$
接下来就是权重更新的梯度，推出第$i$层的梯度后，对权重梯度与偏移的求导可以使用定义法求得到：
$$
\begin{aligned}
\frac{\partial cost}{\partial W^i}&=\frac{\partial cost}{\partial S_{in}^i}*(S_{out}^{i-1})^T\tag{式2}
\end{aligned}
$$
$$
\begin{aligned}
\frac{\partial cost}{\partial B^i}&=\frac{\partial cost}{\partial S_{in}^i}\tag{式3}
\end{aligned}
$$
$\frac{\partial cost}{\partial S_{in}^n}$需要依据矩阵求导的定义法自己求出，求出后，即可依据式1、2、3求出各参数的梯度，关于矩阵求导的定义法，可以查看[快](https://www.cnblogs.com/pinard/p/10750718.html)，[快点我，我等不及了](https://www.cnblogs.com/pinard/p/10773942.html)
