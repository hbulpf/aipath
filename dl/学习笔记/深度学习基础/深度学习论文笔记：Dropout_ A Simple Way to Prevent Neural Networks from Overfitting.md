@[toc]

# 主要工作
神经网络的学习能力很强，很容易过拟合训练数据，为了降低模型的方差，我们常使用集成学习中的bagging思想，即让多个不同的神经网络在大致不同的数据集上训练。对于回归类任务，可以简单的将多个神经网络输出的平均值作为最终的预测结果，对于分类任务，每个神经网络可以投某一类一票，最终选择票数最高的类作为最终的预测结果。

为什么bagging可以降低模型方差？简单来说，一个任务通常有一个普遍规律，而数据集包含普遍规律和噪声两部分，为了提高模型的泛化能力，我们希望模型尽可能的拟合数据集上的普遍规律，而多个模型拟合的共同部分，更有可能是任务的普遍规律。

对神经网络使用bagging有很多困难，具体体现在

 1. 训练一个大型神经网络需要很大的数据集，数据集不够大将导致无法划分足够大的数据子集。
 2. 由于超参数的存在，训练一个大型神经网络通常非常耗时。
 3. 同时运行多个神经网络通常运行效率低下，并且会消耗许多计算资源。

为了解决上述问题，提高神经网络的泛化能力，作者提出了Dropout技术。

<br>

# 什么是Dropout
<br>

## 运作模式
引入超参数P（dropout率），表示一层每个神经元激活的概率。
训练时，对于一个batch，该层每个神经元激活的概率为P，若不激活，则会强制将神经元的输出设置为0。
测试时，该层每个神经元都是激活的，每个神经元的权重乘以P。
具体可以查看下图：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20191002094554574.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RoYWl1ZGE=,size_16,color_FFFFFF,t_70)
<br>

## 灵感来源

<br>

### 从集成学习的角度
假设一个使用了dropout的神经网络有n个神经元，每个神经元都有激活或是不激活两种选项，则一个使用了dropout的神经网络可以看成是一个大小为$2^n$的神经网络集合，只是这$2^n$的神经网络共享权重，对于一个batch，从大小为$2^n$的神经网络集合选出一个神经网络进行训练。因此，训练使用了dropout的神经网络可以看成是不同结构的神经网络在不同数据集上的训练，只是这些不同结构的神经元共享了部分权重，相当于bagging集成学习。

但是测试时，我们无法结合这$2^n$个神经网络的输出，于是作者通过**实践**提出了一种近似方法，即让每个神经元的权重都乘以P（具体查看上一节）。

<br>

### 从有性繁殖的角度
此处不总结有性繁殖的优势，有兴趣可以浏览原论文。
dropout的机制与有性繁殖有点类似，对于不使用dropout的神经网络，一个神经元产生的错误可以依靠其他神经元进行校正，对于使用dropout的神经网络来说，一个神经元需要与随机选择的其余神经元进行合作，意味着一个神经元不能完全依赖于其他神经元来校正误差，从而让该神经元在不依赖于其他神经元的前提下产生更有用的特征信息（对于图像识别而言，例如纹理、形状的有用的特征信息）。

<br>

**此处不总结论文中的实验，实验得出dropout的确可以提高网络的泛化能力**

<br>

# dropout是否是一个好的正则化技术？
对于一个任务而言，大规模数据集中隐藏的规律更有可能任务的规律，小规模数据集中含有的更可能是噪声，即只体现在该小规模数据集上的规律，拟合能力强的模型，往往具有较多的参数，复杂度较高，衡量一种正则化技术的方法之一便是在小规模数据集上使用拟合能力强的模型，拟合能力强的模型更可能拟合噪声，如果在测试集上模型表现优异，则在一定程度上说明该正则化技术可以规避模型拟合噪声，作者比较了不同大小数据集下，同一神经网络（784-1024-1024-2048-10）在MNIST数据集上的测试错误率，结果如下
![在这里插入图片描述](https://img-blog.csdnimg.cn/20191004180520606.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RoYWl1ZGE=,size_16,color_FFFFFF,t_70)
可以看出，在一定规模的数据集上，Dropout可以提高网络的泛化能力，具有一定的正则化效果，实验时选择的dropout率自行查阅论文。

<br>


# 为什么dropout可以提高网络的泛化能力
论文通过实验验证了dropout有助于提高神经网络的泛化能力，并且更进一步的通过实验探讨了为什么dropout有助于提升神经网络的泛化能力。

<br>

## dropout对神经元提取特征的影响
实验使用了具有一层隐藏层，含有256个ReLU单元的自编码器，对比了未使用dropout与使用dropout的情况下，提取的特征情况，具体如下：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20191004172842377.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RoYWl1ZGE=,size_16,color_FFFFFF,t_70)
两类自编码器的测试集准确率基本一致，从上图可以看出，相比于未使用dropout的自编码器，使用了dropout的自编码器神经元提取的特征更具有意义，例如探测到了纹路等。

至于为什么使用了dropout的自编码器测试准确率没有提升，由于我对自编码器不是很了解，在此不做解释，也许与数据集规模有关。

由此可以得出一个猜测：dropout可能让神经元提取的特征更有意义，从而提高网络的泛化能力。

<br>

## dropout对稀疏性的影响
作者通过实验发现使用dropout可以提高网络的稀疏性，稀疏性具有一定的好处，让神经网络模型与生物神经网络更为近似，同时对于输入的微小改变也更为健壮，从而提高泛化能力。

作者利用上一节训练的自编码器，在测试集上随机采样batch，统计其激活函数的输出情况，具体结果如下：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20191004174821452.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RoYWl1ZGE=,size_16,color_FFFFFF,t_70)
横轴表示激活函数值，纵轴表示激活函数值的个数，作者比对了平均激活函数值（一个神经元在随机采样batch上的激活函数平均值）直方图、激活函数值（所有神经元在随机采样batch上的激活函数输出值）直方图。

使用了dropout的自编码器，平均激活函数值直方图明显更偏向于0，激活函数值直方图也更偏向于0。

由此可以得出一个猜测：dropout可能提高网络的稀疏性，从而提高网络的泛化能力。

<br>

# dropout的缺点
由于每次训练都会舍弃部分神经元，可能导致梯度震荡，延长训练时间。

<br>

# 训练dropout网络的技巧

<br>

## 网络大小
dropout每次会舍弃部分神经网络，假设一个神经网络有n个神经元，dropout率为p，则每次训练只存在np个神经元，这会降低模型的复杂度，复杂度不够可能导致无法拟合数据集，因此，假设含有n个神经元的神经网络是最优的，那么应该将p设置为$\frac{n}{p}$，从而在保证拟合数据的前提下提升网络的泛化性能

<br>

## 学习率与动量值
由于dropout可能导致梯度之间相互抵消，因此dropout网络一般使用较大的学习率，动量值一般设置为0.95-0.99之间。

<br>

## Max-Norm 正则化
如果用$w$表示某一层的权重，则该层权重需要在满足$||w||_2<c$的情况下进行优化，其中$c$是一个超参数，这就是Max-Norm 正则化。由于dropout使用较大的学习率以及动量值，有可能导致权重过大，使用Max-Norm正则化可以防止权重爆炸，同时也能提高模型的泛化能力。

<br>

## Dropout率
Dropout引入了一个名为Dropout率的超参数，隐藏层的Dropout率一般为0.5~0.8之间，输入层的Dropout率一般为0.8，Dropout率过小，通常要求该层的神经元个数较大，此时容易出现训练震荡，并且可能出现欠拟合，Dropout率过大可能导致正则化效果不够，需依据神经网络在测试集的准确率来选择最为合适的Dropout。
